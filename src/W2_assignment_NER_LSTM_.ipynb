{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lVecmBAusQKP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.set_random_seed(33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "\n",
        "    with open(file_path,'r') as f:\n",
        "        data = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "0fDBdsC3wfny"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences =  load_data('train_sentences.txt')\n",
        "train_labels = load_data('train_labels.txt')\n",
        "\n",
        "val_sentences =  load_data('val_sentences.txt')\n",
        "val_labels = load_data('val_labels.txt')\n",
        "\n",
        "test_sentences =  load_data('test_sentences.txt')\n",
        "test_labels = load_data('test_labels.txt')"
      ],
      "metadata": {
        "id": "1A38zn-dwg-l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vectorizer(sentences):\n",
        "\n",
        "    # TextVectorization used to build vocab, transform sentence to vector\n",
        "    sentence_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize = None, # By default, standardize = 'lower_and_strip_punctuation', but in NER task we don't need that\n",
        "        split='whitespace', # split tokens by whitespace\n",
        "        max_tokens=5000,    # Maximum number of tokens in the vocabulary\n",
        "    )\n",
        "\n",
        "    # fit TextVectorization on the data\n",
        "    sentence_vectorizer.adapt(sentences)\n",
        "\n",
        "    vocab = sentence_vectorizer.get_vocabulary()\n",
        "\n",
        "    return sentence_vectorizer, vocab"
      ],
      "metadata": {
        "id": "VCfw_FiL5MLe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences[:1000])\n",
        "print(f\"Test vocab size: {len(test_vocab)}\")\n",
        "\n",
        "sentence = \"I like learning new NLP models !\"\n",
        "sentence_vectorized = test_vectorizer(sentence)\n",
        "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQQas_dRO_y",
        "outputId": "af5e497d-8a44-457b-de41-7cb0b033c3eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test vocab size: 4650\n",
            "Sentence: I like learning new NLP models !\n",
            "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sentence: {train_sentences[0]}\")\n",
        "print(f\"Labels: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIJwanulR1As",
        "outputId": "9550b7c4-02a7-4e16-d056-a767bfc296fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tags(labels):\n",
        "\n",
        "    \"get all tags without repetition\"\n",
        "\n",
        "    tag_set = set()\n",
        "\n",
        "    for sent_label in labels:\n",
        "        for tag in sent_label.split(\" \"):\n",
        "            tag_set.add(tag)\n",
        "\n",
        "    tag_list = list(tag_set)\n",
        "    tag_list.sort()\n",
        "\n",
        "    return tag_list"
      ],
      "metadata": {
        "id": "Qzxjynw8SwOz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = get_tags(train_labels)\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "UOE7FNrqUBVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39e13a5-50f4-433e-c969-2b9ac77d8cd2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_map(tags):\n",
        "\n",
        "    \"returns dict for tag, index\"\n",
        "\n",
        "    tag_map = {}\n",
        "\n",
        "    for i, tag in enumerate(tags):\n",
        "        tag_map[tag] = i\n",
        "\n",
        "    return tag_map"
      ],
      "metadata": {
        "id": "G1VVWJHeTumH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_map = make_tag_map(tags)\n",
        "print(tag_map)"
      ],
      "metadata": {
        "id": "45gUW4EeTBpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b970bd-a71a-4921-dc09-3f0f772f26ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_vectorizer(labels, tag_map):\n",
        "\n",
        "    \"transform labels into vectors\"\n",
        "\n",
        "    label_ids = []\n",
        "\n",
        "    for sent_labels in labels:\n",
        "\n",
        "        splitted_labels = sent_labels.split(' ')\n",
        "        label_id = []\n",
        "\n",
        "        for label in splitted_labels:\n",
        "            label_id.append(tag_map[label])\n",
        "\n",
        "        label_ids.append(label_id)\n",
        "\n",
        "    # padding value will be -1\n",
        "    label_ids_padded = tf.keras.utils.pad_sequences(\n",
        "        label_ids, padding = 'post', value = -1)\n",
        "\n",
        "    return label_ids_padded"
      ],
      "metadata": {
        "id": "R3bX3KgH703u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sentence: {train_sentences[5]}\")\n",
        "print(f\"Labels: {train_labels[5]}\")\n",
        "print(f\"Vectorized labels: {label_vectorizer([train_labels[5]], tag_map)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHO73E1oHM2Y",
        "outputId": "5025874c-73a1-4080-d5c9-66a2d339a911"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
            "Labels: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
            "Vectorized labels: [[16 16 16 16 16  3 16 16 16 16  2 16 16 16 16 16 16 16  3 16 16 16 16 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)"
      ],
      "metadata": {
        "id": "xBaCMVUzQ80q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o79eniiIZku5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
        "\n",
        "    sentences_ids = sentence_vectorizer(sentences)\n",
        "    labels_ids = label_vectorizer(labels, tag_map)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "tY5OGclQAYkU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = generate_dataset(\n",
        "    train_sentences, train_labels,sentence_vectorizer,tag_map)\n",
        "\n",
        "val_dataset = generate_dataset(\n",
        "    val_sentences,val_labels,  sentence_vectorizer, tag_map)\n",
        "\n",
        "test_dataset = generate_dataset(\n",
        "    test_sentences, test_labels,  sentence_vectorizer, tag_map)"
      ],
      "metadata": {
        "id": "oIZGspbzQSYU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring information about the training data\n",
        "print(f'The number of outputs is {len(tags)}')\n",
        "# The number of vocabulary tokens (including <PAD>)\n",
        "g_vocab_size = len(vocab)\n",
        "print(f\"Num of vocabulary words in the training set: {g_vocab_size}\")\n",
        "print('The training size is', len(train_dataset))\n",
        "print('The validation size is', len(val_dataset))\n",
        "print('An example of the first sentence is\\n\\t', next(iter(train_dataset))[0].numpy())\n",
        "print('An example of its corresponding label is\\n\\t', next(iter(train_dataset))[1].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jMkJV_h_gj",
        "outputId": "237b250e-4ca6-4b6f-c5f2-157f90f0f037"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of outputs is 17\n",
            "Num of vocabulary words in the training set: 5000\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is\n",
            "\t [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
            "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "An example of its corresponding label is\n",
            "\t [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NER(num_tags, vocab_size, embedding_dim = 50):\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "        # if mask_zero = True, so the index 0 will reserve for padding\n",
        "        # and we need to increase the vocab size by 1 because the first idx is reserved\n",
        "        tf.keras.layers.Embedding(\n",
        "            input_dim = vocab_size + 1,\n",
        "            output_dim = embedding_dim,\n",
        "            mask_zero = True),\n",
        "\n",
        "        tf.keras.layers.LSTM(\n",
        "            units = embedding_dim, return_sequences = True),\n",
        "\n",
        "        tf.keras.layers.Dense(\n",
        "            num_tags, activation = tf.nn.log_softmax)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "erKuIFZcjHME"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    \"the loss function with ignoring the padding\"\n",
        "\n",
        "    loss_fun = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits = True, # output is log probability, so it's logits\n",
        "        ignore_class = -1,  # the padding value in y_true\n",
        "    )\n",
        "\n",
        "    loss = loss_fun(y_true, y_pred)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "45G5kUAzT5fy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = np.array([0,1,2,0])\n",
        "\n",
        "predicted_logits = np.array([[0.4,0.6,0.3] , [0.22,0.79,0.2], [1, 4.5,0.4], [0.4,0.4,7.2]])\n",
        "\n",
        "masked_loss(true_labels, predicted_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c6BFMfVUcFH",
        "outputId": "b72d3b1b-affd-40d4-962b-e8e18d68b26c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3.2097778>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(y_true, y_pred):\n",
        "\n",
        "    \"get accuracy with ignoring for padding values in true labels\"\n",
        "\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, -1), dtype = tf.int32)\n",
        "\n",
        "    y_pred_labels = tf.argmax(y_pred, axis = -1)\n",
        "    y_pred_labels = tf.cast(y_pred_labels, dtype = tf.int32)\n",
        "\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "    matches_true_pred = tf.equal(y_true, y_pred_labels)\n",
        "    matches_true_pred = tf.cast(matches_true_pred,tf.int32)\n",
        "    matches_true_pred *= mask\n",
        "\n",
        "    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)\n",
        "\n",
        "    return masked_acc"
      ],
      "metadata": {
        "id": "2k6rvvRBaFCp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = np.array([0,1,2,0, -1])\n",
        "\n",
        "predicted_logits = np.array([\n",
        "    [0.1,0.6,0.36],\n",
        "    [0.1,0.7,0.1],\n",
        "    [0.1, 6.5,0.4],\n",
        "    [1.4,0.4,0.2],\n",
        "    [0.1,0.6,9.4]\n",
        "    ])\n",
        "\n",
        "print(masked_accuracy(true_labels, predicted_logits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flzCz0iRtI8t",
        "outputId": "ea8f2cfb-62f9-489c-9e4f-c8c65e11d8fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.5, shape=(), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_tags = len(tag_map)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = NER(n_tags, vocab_size, embedding_dim = 50)\n",
        "model.build(input_shape=(None, None))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "y6JsGfXlyBJg",
        "outputId": "5a236241-8348-42b3-9031-a8434df5cec7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)            │         \u001b[38;5;34m250,050\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)            │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)            │             \u001b[38;5;34m867\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">250,050</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">867</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,117\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,117</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m271,117\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,117</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A note on padding**\n",
        "\n",
        "let's check now how padding does not affect the model's output. Of course the output dimension will change. If ten zeros are added at the end of the tensor, then the resulting output dimension will have 10 more elements (more specifically, 10 more arrays of length 17 each). However, those are removed from any calculation further on, so it won't impact at all the model's performance and training. You will be using the function tf.expand_dims."
      ],
      "metadata": {
        "id": "JbsyPH7GSkBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.expand_dims(np.array([545, 467, 896]), axis = 0)\n",
        "\n",
        "x_padded = tf.expand_dims(np.array([545, 467, 896, 0, 0, 0]), axis = 0)\n",
        "\n",
        "\n",
        "pred_x = model(x)\n",
        "pred_x_padded = model(x_padded)\n",
        "print(f'x shape: {pred_x.shape}\\nx_padded shape: {pred_x_padded.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SojbEa3syb8T",
        "outputId": "c3dab2ef-886b-4964-8626-dd1c6a88f87d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: (1, 3, 17)\n",
            "x_padded shape: (1, 6, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if pred_x the same as pred_x_padded when we neglect padding vectors\n",
        "np.allclose(pred_x, pred_x_padded[:, :3, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfL3EDoAK3To",
        "outputId": "c13fa048-24e4-4d8f-c6d1-5740391e3c95"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true =tf.expand_dims([16, 6, 12], axis = 0)\n",
        "y_true_padded =tf.expand_dims([16,6,12,-1,-1,-1], axis = 0) # Remember you mapped the padded values to -1 in the labels\n",
        "print(f\"masked_loss is the same: {np.allclose(masked_loss(y_true, pred_x), masked_loss(y_true_padded, pred_x_padded))}\")\n",
        "print(f\"masked_accuracy is the same: {np.allclose(masked_accuracy(y_true, pred_x), masked_accuracy(y_true_padded, pred_x_padded))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kJ4oQtdLWbF",
        "outputId": "421ec87f-7436-4f9e-8718-1232d86523d7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_loss is the same: True\n",
            "masked_accuracy is the same: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = masked_loss,\n",
        "              optimizer = tf.keras.optimizers.Adam(0.01),\n",
        "              metrics = [masked_accuracy])"
      ],
      "metadata": {
        "id": "KthoLCiTMCN1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = val_dataset.batch(64)\n",
        "train_dataset = train_dataset.batch(64)\n",
        "test_dataset = test_dataset.batch(64)"
      ],
      "metadata": {
        "id": "JP0chAs5OJYj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset,\n",
        "          validation_data = val_dataset,\n",
        "          shuffle = True,\n",
        "          epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqGF4XhOO3Cs",
        "outputId": "eb7c98dc-3852-4f1c-ad32-ef87759b9ee6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.4659 - masked_accuracy: 0.8934 - val_loss: 0.1675 - val_masked_accuracy: 0.9497\n",
            "Epoch 2/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.1697 - masked_accuracy: 0.9494 - val_loss: 0.1592 - val_masked_accuracy: 0.9508\n",
            "Epoch 3/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1541 - masked_accuracy: 0.9523 - val_loss: 0.1600 - val_masked_accuracy: 0.9510\n",
            "Epoch 4/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.1439 - masked_accuracy: 0.9546 - val_loss: 0.1596 - val_masked_accuracy: 0.9509\n",
            "Epoch 5/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.1361 - masked_accuracy: 0.9566 - val_loss: 0.1619 - val_masked_accuracy: 0.9503\n",
            "Epoch 6/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1303 - masked_accuracy: 0.9583 - val_loss: 0.1637 - val_masked_accuracy: 0.9495\n",
            "Epoch 7/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.1262 - masked_accuracy: 0.9593 - val_loss: 0.1686 - val_masked_accuracy: 0.9481\n",
            "Epoch 8/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.1222 - masked_accuracy: 0.9603 - val_loss: 0.1697 - val_masked_accuracy: 0.9483\n",
            "Epoch 9/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1184 - masked_accuracy: 0.9617 - val_loss: 0.1758 - val_masked_accuracy: 0.9480\n",
            "Epoch 10/10\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.1147 - masked_accuracy: 0.9628 - val_loss: 0.1753 - val_masked_accuracy: 0.9478\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x790dcfd7add0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'weights.keras'\n",
        "model.save(save_path)"
      ],
      "metadata": {
        "id": "vwAbeWoBTC1b"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents_ids = sentence_vectorizer(test_sentences)\n",
        "true_test_labels = label_vectorizer(test_labels,tag_map)\n",
        "\n",
        "y_pred_test = model.predict(test_sents_ids)\n",
        "\n",
        "test_acc = masked_accuracy(true_test_labels, y_pred_test)\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpT6YcEnvRVk",
        "outputId": "2ec8c3dc-5b26-47b4-8fe5-cad889b79ede"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=0.9472714005914206>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
        "\n",
        "    sent_ids = sentence_vectorizer(sentence)\n",
        "    sent_ids = tf.expand_dims(sent_ids,axis=0)\n",
        "\n",
        "    pred_logits = model(sent_ids)\n",
        "    pred_labels = tf.argmax(pred_logits, axis = -1)\n",
        "\n",
        "    labels = list(tag_map.keys())\n",
        "    pred = []\n",
        "\n",
        "    for label in pred_labels.numpy()[0]:\n",
        "        tag = labels[label]\n",
        "        pred.append(tag)\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "Z6TGrb6pxGOn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"OpenAI and Google are great companies to work for in US\"\n",
        "\n",
        "predict(sentence, model, sentence_vectorizer, tag_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbpWRMmuz-e-",
        "outputId": "336cd659-6dea-4e0c-a729-8558fce6fed4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}